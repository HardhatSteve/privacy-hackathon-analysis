version: '3.8'

# =============================================================================
# Auction Data Pipeline - Docker Compose Configuration
# =============================================================================
# This file orchestrates all services for local development.
# Run: docker compose up -d
# =============================================================================

x-spark-common: &spark-common
  image: bitnami/spark:3.5
  environment:
    - SPARK_MODE=worker
    - SPARK_MASTER_URL=spark://spark-master:7077
    - SPARK_WORKER_MEMORY=2G
    - SPARK_WORKER_CORES=2
  networks:
    - auction-network

services:
  # ===========================================================================
  # MESSAGE BROKER - Apache Kafka (KRaft Mode - No Zookeeper)
  # ===========================================================================
  kafka:
    image: bitnami/kafka:3.6
    container_name: kafka
    ports:
      - "9092:9092"
      - "9094:9094"
    environment:
      # KRaft Configuration
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,EXTERNAL://localhost:9094
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      # Topic Configuration
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      - KAFKA_CFG_NUM_PARTITIONS=3
      - KAFKA_CFG_DEFAULT_REPLICATION_FACTOR=1
      - KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1
      - KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR=1
      # Performance
      - KAFKA_CFG_LOG_RETENTION_HOURS=168
      - KAFKA_CFG_LOG_SEGMENT_BYTES=1073741824
      - KAFKA_HEAP_OPTS=-Xmx1G -Xms512M
    volumes:
      - kafka-data:/bitnami/kafka
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions.sh --bootstrap-server localhost:9092"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - auction-network

  # ===========================================================================
  # KAFKA UI - Web Interface for Kafka Management
  # ===========================================================================
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    ports:
      - "8080:8080"
    environment:
      - KAFKA_CLUSTERS_0_NAME=auction-cluster
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092
      - DYNAMIC_CONFIG_ENABLED=true
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - auction-network

  # ===========================================================================
  # POSTGRESQL - Operational Database (Silver Layer)
  # ===========================================================================
  postgres:
    image: postgres:15-alpine
    container_name: postgres
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=auction
      - POSTGRES_PASSWORD=auction123
      - POSTGRES_DB=auction
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./sql/init.sql:/docker-entrypoint-initdb.d/01-init.sql
      - ./sql/silver_schema.sql:/docker-entrypoint-initdb.d/02-silver.sql
      - ./sql/gold_schema.sql:/docker-entrypoint-initdb.d/03-gold.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U auction -d auction"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - auction-network

  # ===========================================================================
  # APACHE SPARK - Processing Engine
  # ===========================================================================
  spark-master:
    image: bitnami/spark:3.5
    container_name: spark-master
    ports:
      - "7077:7077"
      - "4040:8080"  # Spark UI
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./src:/opt/spark-apps/src
      - ./config:/opt/spark-apps/config
      - spark-logs:/opt/spark/logs
    networks:
      - auction-network

  spark-worker:
    <<: *spark-common
    container_name: spark-worker
    ports:
      - "8082:8081"  # Worker UI
    depends_on:
      - spark-master
    volumes:
      - ./src:/opt/spark-apps/src
      - spark-logs:/opt/spark/logs

  # ===========================================================================
  # APACHE AIRFLOW - Workflow Orchestration
  # ===========================================================================
  airflow-init:
    image: apache/airflow:2.8.0-python3.11
    container_name: airflow-init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db init
        airflow users create \
          --username airflow \
          --password airflow \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com || true
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://auction:auction123@postgres:5432/auction
      - AIRFLOW__CORE__FERNET_KEY=your-fernet-key-here-replace-me-32bytes
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    volumes:
      - ./dags:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - auction-network

  airflow-webserver:
    image: apache/airflow:2.8.0-python3.11
    container_name: airflow-webserver
    command: webserver
    ports:
      - "8081:8080"
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://auction:auction123@postgres:5432/auction
      - AIRFLOW__CORE__FERNET_KEY=your-fernet-key-here-replace-me-32bytes
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
    volumes:
      - ./dags:/opt/airflow/dags
      - ./src:/opt/airflow/src
      - airflow-logs:/opt/airflow/logs
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - auction-network

  airflow-scheduler:
    image: apache/airflow:2.8.0-python3.11
    container_name: airflow-scheduler
    command: scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://auction:auction123@postgres:5432/auction
      - AIRFLOW__CORE__FERNET_KEY=your-fernet-key-here-replace-me-32bytes
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    volumes:
      - ./dags:/opt/airflow/dags
      - ./src:/opt/airflow/src
      - airflow-logs:/opt/airflow/logs
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    networks:
      - auction-network

  # ===========================================================================
  # METABASE - Business Intelligence & Visualization
  # ===========================================================================
  metabase:
    image: metabase/metabase:latest
    container_name: metabase
    ports:
      - "3000:3000"
    environment:
      - MB_DB_TYPE=postgres
      - MB_DB_DBNAME=auction
      - MB_DB_PORT=5432
      - MB_DB_USER=auction
      - MB_DB_PASS=auction123
      - MB_DB_HOST=postgres
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - auction-network

  # ===========================================================================
  # LOCALSTACK - AWS Services Emulation (S3, etc.)
  # ===========================================================================
  localstack:
    image: localstack/localstack:latest
    container_name: localstack
    ports:
      - "4566:4566"
      - "4510-4559:4510-4559"
    environment:
      - SERVICES=s3,sqs,kinesis
      - DEBUG=0
      - DATA_DIR=/var/lib/localstack/data
      - DOCKER_HOST=unix:///var/run/docker.sock
    volumes:
      - localstack-data:/var/lib/localstack
      - /var/run/docker.sock:/var/run/docker.sock
      - ./scripts/init-localstack.sh:/etc/localstack/init/ready.d/init-aws.sh
    networks:
      - auction-network

  # ===========================================================================
  # REDIS - Caching Layer (Optional but recommended for production)
  # ===========================================================================
  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    networks:
      - auction-network

# =============================================================================
# VOLUMES
# =============================================================================
volumes:
  kafka-data:
    driver: local
  postgres-data:
    driver: local
  spark-logs:
    driver: local
  airflow-logs:
    driver: local
  localstack-data:
    driver: local
  redis-data:
    driver: local

# =============================================================================
# NETWORKS
# =============================================================================
networks:
  auction-network:
    driver: bridge
    name: auction-network
